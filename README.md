# ESA-MNE Challenge: Multinational Enterprise Profiling Pipeline

This repository implements a complete, modular, and intelligent pipeline to **profile multinational enterprises (MNEs)** using a combination of **public web data**, **PDF extraction**, and **LLM-assisted classification**. It is designed to produce structured data for both the **discovery** and **extraction** challenges of the ESA-MNE competition.

---

## ğŸš€ Project Overview

The pipeline collects and consolidates data for a set of MNEs using the following sources:

* ğŸŒ **Wikipedia & Wikidata** â€” General company information.
* ğŸ“‰ **Yahoo Finance** â€” Public financial records and business descriptions.
* ğŸ“„ **Annual Reports** â€” Extracted from PDFs via web search and LLM processing.
* ğŸ§ **Official Registers** â€” Governmental registries (e.g., Franceâ€™s SIREN) for validated activity codes.

The final output includes:

* `discovery_submission.csv` â€” Provenance of discovered sources.
* `extraction_submission.csv` â€” Structured company facts: turnover, employees, assets, website, activity and country of headquarters.

> ğŸ” **Try It Live**: You can explore this pipeline interactively with the [Multinational Enterprise Explorer App](https://mne-ui.lab.sspcloud.fr/). It's powered by Streamlit and includes search, extraction, and classification steps in real time.

---

## ğŸ§½ Pipeline Architecture

![Pipeline Architecture](public/diag-challenge-mne-extraction.png)

## ğŸ“ Repository Structure

```
â””â”€â”€ esa-mne-challenge/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ renovate.json
    â”œâ”€â”€ setup.sh
    â”œâ”€â”€ uv.lock
    â”œâ”€â”€ .env
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ .python-version
    â”œâ”€â”€ cache/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ discovery/
    â”‚   â””â”€â”€ extraction/
    â”œâ”€â”€ kubernetes/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ app.py                   # Streamlit app to deploy the pipeline
    â”‚   â”œâ”€â”€ build_vector_db.py       # Script that construct the Vector DB
    â”‚   â”œâ”€â”€ run_pipeline.py          # Main pipeline orchestration
    â”‚   â”œâ”€â”€ cache/                   # Local cache for report URLs and tickers
    â”‚   â”œâ”€â”€ common/                  # Shared utilities for both discovering and extraction challenges
    â”‚   â”œâ”€â”€ config/                  # Logging, env setup
    â”‚   â”œâ”€â”€ extractors/              # Source-specific structured data extractors
    â”‚   â”œâ”€â”€ fetchers/                # Source fetchers
    â”‚   â”œâ”€â”€ nace_classifier/         # RAG-based NACE classification
    â”‚   â””â”€â”€ vector_db/               # Vector DB loading logic
    â””â”€â”€ .github/workflows/

```

---

## ğŸ› ï¸ Setup Instructions

### 1. Environment

```bash
uv sync
```

### 2. Environment Variables

Copy and configure `.env.example`:

```env
OPENAI_API_KEY=your-api-key
```

---

## â–¶ï¸ Running the Pipeline

To try the pipeline, the easiest is that you test it via the [App](https://mne-ui.lab.sspcloud.fr/)

If you wish to run the pipeline locally to generate submission files for the challenge, youâ€™ll need to configure Langfuse for LLM trace logging, set up an OpenAI client, and connect to the Qdrant vector database. The simplest way to get started is by using the [**SSPCloud** platform](https://datalab.sspcloud.fr/), and Iâ€™d be happy to assist with setup if needed.


To run the pipeline, execute the following command:

```bash
uv python src/run_pipeline.py
```

Results will be saved as:

* `discovery_submission.csv`
* `extraction_submission.csv`

---

## ğŸ” Major Components

### ğŸ§  `WikipediaExtractor`

* Scrapes infobox, Wikidata, and article summary.
* Resolves conflicting info via quality-first arbitration.

### ğŸ“‰ `YahooExtractor`

* Uses `yfinance` to extract revenue, assets, employees, and activity.
* Normalizes currency and domain names.

### ğŸ“„ `AnnualReportFetcher`

* Searches for company PDFs using Google/DDG.
* Validates links and selects final URL using LLM (Langfuse).
* Caches results in `/tmp/cache/reports_cache.json`.

### ğŸ“ƒ `PDFExtractor`

* Extracts specific values (e.g., revenue) from PDF pages via:

  * Keyword-based page selection
  * Langfuse prompt + LLM structuring
  * Country/currency normalization

### ğŸ§ `OfficialRegisterFetcher`

* Country-specific registry data (e.g., Franceâ€™s SIREN).
* Retrieves official national IDs and activity codes.

### ğŸ§ `NACEClassifier`

* Uses a vector database of NACE descriptions (retrieved via `get_vector_db()`).
* Prompt-instructed LLM selects most appropriate NACE code.
* Appends correct section letter using `mapping.json`.

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¥ Author

* Thomas Faria

---
