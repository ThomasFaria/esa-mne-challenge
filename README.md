# ESA-MNE Challenge: Multinational Enterprise Profiling Pipeline

This repository implements a complete, modular, and intelligent pipeline to **profile multinational enterprises (MNEs)** using a combination of **public web data**, **PDF extraction**, and **LLM-assisted classification**. It is designed to produce high-quality structured data for both the **discovery** and **extraction** challenges of the ESA-MNE competition.

---

## ğŸš€ Project Overview

The pipeline collects and consolidates data for a set of MNEs using the following sources:

* ğŸŒ **Wikipedia & Wikidata** â€” General company information.
* ğŸ“‰ **Yahoo Finance** â€” Public financial records and business descriptions.
* ğŸ“„ **Annual Reports** â€” Extracted from PDFs via web search and LLM processing.
* ğŸ§ **Official Registers** â€” Governmental registries (e.g., Franceâ€™s SIREN) for validated activity codes.

The final output includes:

* `discovery_submission.csv` â€” Provenance of discovered sources.
* `extraction_submission.csv` â€” Structured company facts: turnover, employees, assets, website, activity and country of headquarters.

> ğŸ” **Try It Live**: You can explore this pipeline interactively with the [Multinational Enterprise Explorer App](https://mne-ui.lab.sspcloud.fr/). It's powered by Streamlit and includes search, extraction, and classification steps in real time.

---

## ğŸ§½ Pipeline Architecture

```text
[ MNE List (CSV) ]
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      run_pipeline.py       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Wikipedia   â”‚     Yahoo     â”‚ Official Reg. â”‚
â”‚  Extractor   â”‚  Extractor    â”‚   Fetcher     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
       Merge & Deduplicate
             â–¼
       [ Web Search: Annual Report ]
             â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ PDF Extraction (LLM + PDF) â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   NACE Classification      â”‚
     â”‚ (VectorDB + LLM + Mapping) â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â–¼
     Write Final Submissions

---

## ğŸ“ Repository Structure

```
â””â”€â”€ esa-mne-challenge/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ renovate.json
    â”œâ”€â”€ setup.sh
    â”œâ”€â”€ uv.lock
    â”œâ”€â”€ .env
    â”œâ”€â”€ .pre-commit-config.yaml
    â”œâ”€â”€ .python-version
    â”œâ”€â”€ cache/
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ discovery/
    â”‚   â””â”€â”€ extraction/
    â”œâ”€â”€ kubernetes/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ app.py                   # Streamlit app to deploy the pipeline
    â”‚   â”œâ”€â”€ build_vector_db.py       # Script that construct the Vector DB
    â”‚   â”œâ”€â”€ run_pipeline.py          # Main pipeline orchestration
    â”‚   â”œâ”€â”€ cache/                   # Local cache for report URLs and tickers
    â”‚   â”œâ”€â”€ common/                  # Shared utilities for both discovering and extraction challenges
    â”‚   â”œâ”€â”€ config/                  # Logging, env setup
    â”‚   â”œâ”€â”€ extractors/              # Source-specific structured data extractors
    â”‚   â”œâ”€â”€ fetchers/                # Source fetchers
    â”‚   â”œâ”€â”€ nace_classifier/         # RAG-based NACE classification
    â”‚   â””â”€â”€ vector_db/               # Vector DB loading logic
    â””â”€â”€ .github/workflows/

```

---

## ğŸ› ï¸ Setup Instructions

### 1. Environment

```bash
# Python version (e.g. from .python-version)
pyenv install 3.10.9
pyenv local 3.10.9

# Install dependencies (Poetry or pip+pyproject)
pip install -r requirements.txt
```

### 2. Environment Variables

Copy and configure `.env.example`:

```env
OPENAI_API_KEY=your-api-key
```

---

## â–¶ï¸ Running the Pipeline

Go to the app.


to produce submission file for the challenge need to run. Need to configure Langfuse to track LLM traces, OpenAI client, Qdrant Vector database => so the easiest is to deploy this on Onyxia.


```bash
uv python src/run_pipeline.py
```

Results will be saved as:

* `discovery_submission.csv`
* `extraction_submission.csv`

---

## ğŸ” Major Components

### ğŸ§  `WikipediaExtractor`

* Scrapes infobox, Wikidata, and article summary.
* Resolves conflicting info via quality-first arbitration.

### ğŸ“‰ `YahooExtractor`

* Uses `yfinance` to extract revenue, assets, employees, and activity.
* Normalizes currency and domain names.

### ğŸ“„ `AnnualReportFetcher`

* Searches for company PDFs using Google/DDG.
* Validates links and selects final URL using LLM (Langfuse).
* Caches results in `/tmp/cache/reports_cache.json`.

### ğŸ“ƒ `PDFExtractor`

* Extracts specific values (e.g., revenue) from PDF pages via:

  * Keyword-based page selection
  * Langfuse prompt + LLM structuring
  * Country/currency normalization

### ğŸ§ `OfficialRegisterFetcher`

* Country-specific registry data (e.g., Franceâ€™s SIREN).
* Retrieves official national IDs and activity codes.

### ğŸ§ `NACEClassifier`

* Uses a vector database of NACE descriptions (retrieved via `get_vector_db()`).
* Prompt-instructed LLM selects most appropriate NACE code.
* Appends correct section letter using `mapping.json`.

---

## ğŸ“¦ Deployment

```bash
# Build Docker image (via GitHub Actions or locally)
docker build -t esa-mne-pipeline .
```

Optional Kubernetes manifests are provided under `/kubernetes`.

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¥ Author

* Thomas Faria

---




kubectl cp cache/reports_cache.json user-tfaria/mne-ui-5566545478-nl8lb:/tmp/cache/reports_cache.json
kubectl cp cache/tickers_cache.json user-tfaria/mne-ui-5566545478-nl8lb:/tmp/cache/tickers_cache.json
